{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pcxusFFvAC4A",
        "oZqlt-RYAD9W",
        "4c8lbnCpAKR7",
        "8tAUAu98l-E6",
        "o39LEL0PmKJa",
        "2gjU-yrmmo8I",
        "OWcy51lCOuTv",
        "ONPG_rNpOuT8",
        "zgSTFXdcOuUc",
        "ebaricL9O20B",
        "WWZ0GCbCO20L",
        "XXsiMoefO21J",
        "UQvS83K_PBJh",
        "CjklXwZLPBJw",
        "qP7c_QZcPBK0",
        "jmQzRWnBAUip",
        "RVDxacbDW-Wo",
        "PxBhzro9WOsp",
        "SZOg8AbeAaub",
        "FFOkaq3vXZMh",
        "-1tL9P5BAhWS",
        "-DeWc_D7WVBl",
        "yVTVl_GpWToR",
        "bISArgNWBxR-",
        "Hf72T4GKXzb_",
        "v5M3iGK8PZ3Y",
        "jnxNSUhdPZ3u",
        "wt8Se1hXPZ4C",
        "vAuIYctBB1ZX",
        "fJ84H5gZYIgZ"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/federicomilani/Javascript-Part-I/blob/main/%5BPractice_Notebook%5D_AfterWork_Data_Wrangling_with_Python_Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f9HbIqC9Glg"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Colaboratory, you will need to make a copy of it. Go to File > Save a Copy in Drive. You can then use the new copy that will appear in the new tab.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Z5SVjn9XxQ"
      },
      "source": [
        "# Practice Notebook: Data Wrangling with Python Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxusFFvAC4A"
      },
      "source": [
        "## Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-wn-gNt8_0R"
      },
      "source": [
        "# Pre-requisite 1\n",
        "# ---\n",
        "# The first thing that we will do in this notebook is\n",
        "# to import the pandas library for data manipulation.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj8z63_CMpmb"
      },
      "source": [
        "# Pre-requisite 2\n",
        "# ---\n",
        "# We will also import the numpy library which\n",
        "# will allow us to perform scientific computations.\n",
        "# ---\n",
        "# For those who are not familiar with what a library is,\n",
        "# A library is a collection of related pieces of code that\n",
        "# have been compiled and stored together for reuse.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZqlt-RYAD9W"
      },
      "source": [
        "## 1. Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwZMMVOTAHbV"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Loading a dataset (csv file) from a url\n",
        "# ---\n",
        "# Dataset url (csv file) = http://bit.ly/IrisDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Reading our csv file from the given url and storing it to a dataframe.\n",
        "# A data frame is a two-dimensional data structure that is used to\n",
        "# represent a table of data with rows and columns.\n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv(\"http://bit.ly/IrisDataset\")\n",
        "\n",
        "# Previewing the first 5 records\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o_jT5Uyqx1G"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# We can also load A CSV Into pandas as shown below\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/CitiesDataset1\n",
        "# ----\n",
        "# Instructions:\n",
        "# 1. Visit the above dataset url with a browser. Then download the file.\n",
        "# 2. Within this notebook you will need to have the Table of contents left sidebar open.\n",
        "#    If its not open, you can open the sidebar clicking at the top of the notebook View -> Table of Contents.\n",
        "# 3. Once its open click on the Files tab within the sidebar (this is on the farmost right).\n",
        "# 4. Then upload the downloaded file to this location/ Or drag the file\n",
        "# 5. Once uploaded you will use the name of the file to open the file as shown below.\n",
        "#    You can also store in a directory/folder then reference that directory while reading the file.\n",
        "#    i.e. the file cities stored in a directory named finance, then the reference pd.read_csv(\"/finance/cities.csv\")\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Let's read the cities csv file\n",
        "df_cities = pd.read_csv(\"cities.csv\")\n",
        "\n",
        "# Previewing the first five records\n",
        "df_cities.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoQlxZpAM3Q"
      },
      "source": [
        "# Challenge 1a\n",
        "# ---\n",
        "# Question: Load the following hotels dataset and preview the first 5 records.\n",
        "# Hint: Use a different dataframe that the one used in the example dataset\n",
        "#       i.e. using the dataframe name \"hotel_df\" rather using the dataframe name \"df\"\n",
        "#       If you use the same dataframe names for the examples and challenges you\n",
        "#       might experience a crash.\n",
        "#       We will be using this dataset for most of the challenges.\n",
        "#     : To work quickly on theses challenges, copy paste example code and modify the your need.\n",
        "# ---\n",
        "# Dataset url (csv file) = https://bit.ly/HotelBookingsDB\n",
        "# This data set contains booking information for a city hotel and a resort hotel,\n",
        "# and includes information such as when the booking was made, length of stay,\n",
        "# the number of adults, children, and/or babies, and the number of available\n",
        "# parking spaces, among other things.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y1poor54Wa8"
      },
      "source": [
        "# Challenge 1b\n",
        "# ---\n",
        "# Question: Load a dataset (excel file) from the url below\n",
        "# Hint: - Use the read_excel() function rather than using the read_csv() function\n",
        "#       - Use a new dataframe windmill_df\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/WindmillDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c8lbnCpAKR7"
      },
      "source": [
        "## 2. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsUVqRqbATr0"
      },
      "source": [
        "# Example 2a\n",
        "# ---\n",
        "# Determining the no. of records in the dataset\n",
        "# NB: We will use the above loaded dataset in example 1\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Shape returns no. or records/instances (left) and columns/variables (right)\n",
        "#\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvZTev3C37-2"
      },
      "source": [
        "# Challenge 2a\n",
        "# ---\n",
        "# Question: Determine the no. of records in the our hotels dataset.\n",
        "# NB: last() gives us the last 5 records/instances\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWVV3zfwGgHK"
      },
      "source": [
        "# Example 2b\n",
        "# ---\n",
        "# Previewing the last few records/instances of our dataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s96T5zy34jZh"
      },
      "source": [
        "# Challenge 2c\n",
        "# ---\n",
        "# Question: Preview the last few records in the hotels dataset.\n",
        "# Hint = Use the dataframe you created for this dataset.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0snrMicC4urE"
      },
      "source": [
        "# Challenge 2d\n",
        "# ---\n",
        "# Question: Preview a sample of 10 records from the hotels dataset.\n",
        "# Hint: Use the dataframe you created for this dataset.\n",
        "#     : Use the sample() function, and add no. of desired records as the parameter.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx2kj5ZHGu-y"
      },
      "source": [
        "# Example 2c\n",
        "# ---\n",
        "# Checking the datatypes of df variables (columns)\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3koGy_RATgd"
      },
      "source": [
        "# Challenge 2e\n",
        "# ---\n",
        "# Question: Check the datatypes of the hotels dataset.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tAUAu98l-E6"
      },
      "source": [
        "## 3. Standardisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o39LEL0PmKJa"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql23JRMAlvv2"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Renaming column names\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Reading our dataset from the url\n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv('http://bit.ly/DataCleaningDataset')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkKMMGbdztE7"
      },
      "source": [
        "# We specify the character ; as our separator so that we can\n",
        "# be able to ready the above file that has \";\" as a separator\n",
        "# for our columns. We should note that many dataset may note have\n",
        "# such a structure, but if we come across this kind of a scenario\n",
        "# this is how we would read our file.\n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv('http://bit.ly/DataCleaningDataset', ';')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjbZr_6MXhLb"
      },
      "source": [
        "# Example 1a\n",
        "# ---\n",
        "# We now rename our columns.\n",
        "# We can use this method if we have many column names.\n",
        "# We will use the str.strip(), str.lower(), str.replace() functions\n",
        "# to ensure that our column names are in lowercase format that we easily\n",
        "# reference while performing further analysis.\n",
        "# ---\n",
        "# str.strip() - This fuction is used to remove leading and trailing characters.\n",
        "# str.lower() - This function is used to convert all characters to lowercase\n",
        "# str.replace() - This fuction is used to replace text with some other text.\n",
        "# ---\n",
        "#\n",
        "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
        "\n",
        "# Then preview our resulting dataframe\n",
        "# ---\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcZ63_4lWDRG"
      },
      "source": [
        "# Example 1b\n",
        "# ---\n",
        "# Alternatively, we can rename column names in a dataframe manually by\n",
        "# specifying new column names that would replace the original column names.\n",
        "# You should note that this method is cumbersome when\n",
        "# the no. of features/varibles/columns become large.\n",
        "# ---\n",
        "#\n",
        "\n",
        "# To demonstrate this, we will need to import our dataset again\n",
        "# so that we work with our original dataset.\n",
        "# ---\n",
        "#\n",
        "df = pd.read_csv('http://bit.ly/DataCleaningDataset', ';')\n",
        "\n",
        "# We then specify our columns names, store them in a list, then afterwards\n",
        "# assign this list to the original column names.\n",
        "\n",
        "# Lets first see our original column names below\n",
        "# ---\n",
        "#\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFccEuW9SVGY"
      },
      "source": [
        "# We then perform our column values replacement as shown below\n",
        "# ---\n",
        "#\n",
        "df.columns = ['name', 'city', 'country', 'height', 'weight', 'account_a', 'account_b', 'total_account']\n",
        "\n",
        "# We then preview our dataframe as shown\n",
        "# ---\n",
        "#\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g_2_fG1md0X"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# We can also perform string conversion to a particular column.\n",
        "# This allows us to have uniformity across all values of a column.\n",
        "# In this example, we will convert the values of the column \"city\" to lower case values.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Lets convert the city column to comprise of only lowercase characters\n",
        "# ---\n",
        "#\n",
        "df['city'] = df['city'].str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8bgFBh2JAs3"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# We can also perform types of conversion that we would want i.e. metric conversion.\n",
        "# In this example, we will convert our height values to centimeters noting\n",
        "# that 1 inch = 2.54 cm.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "#\n",
        "\n",
        "# We can perform our conversion across the column that we would want\n",
        "# then replace the column with the outcome of our conversion.\n",
        "# ---\n",
        "#\n",
        "df['height'] = df['height'] * 2.54\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xqz-EHilp9"
      },
      "source": [
        "# Example 4\n",
        "# ---\n",
        "# We can also perform other types of conversion such as datatype conversion as shown\n",
        "# in the next cell.\n",
        "# ---\n",
        "\n",
        "\n",
        "# But before we do that, let's first determine the column/feature datatypes\n",
        "# ---\n",
        "#\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVwcfVsfjW7G"
      },
      "source": [
        "# Then perform a conversion by converting our column/feature\n",
        "# through the use of the apply() function, passing the numerical\n",
        "# type provided by numpy.\n",
        "# To get an understanding of other datatypes provided by numpy we can visit:\n",
        "# https://docs.scipy.org/doc/numpy/user/basics.types.html\n",
        "# ---\n",
        "# Other\n",
        "# ---\n",
        "#\n",
        "df['height'] = df['height'].apply(np.int64)\n",
        "\n",
        "# Let's now check whether our conversion happened by checking our updated datatypes\n",
        "# ---\n",
        "#\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gjU-yrmmo8I"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M81HSax-W3-T"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Convert the variables account_a and account_b to integer datatype.\n",
        "# ---\n",
        "# Hint: You can refer to the df dataframe in the example.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGSOBG7im9Ch"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Convert the given weight feature in the dataset from pounds to grams.\n",
        "# ---\n",
        "# Hint: 1 pound = 453.592 grams\n",
        "#     : You can refer to the df dataframe in the example.\n",
        "# ---\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWcy51lCOuTv"
      },
      "source": [
        "## 4. Syntax Errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONPG_rNpOuT8"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JgjmQlAOuUA"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# While performing our analysis, we can get to a point where we need to\n",
        "# fix spelling mistakes or typos. This example will show us how we can\n",
        "# go about this.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Let's replacing any value \"GERMANYY\" with the correct value \"GERMANY\".\n",
        "# We use the string replace() function to perform our operation as shown.\n",
        "# ---\n",
        "#\n",
        "df['country'] = df['country'].str.replace('GERMANYY', 'GERMANY')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cViv8asNOuUM"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# We can also decide to strip or remove leading spaces (space infront)\n",
        "# and trailing spaces (spaces at the end) by using the string strip() function\n",
        "# covered in this example.\n",
        "# ---\n",
        "# Dataset = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# We first load our dataframe column with the intention to observing leading\n",
        "# and trailing spaces in the city column\n",
        "# ---\n",
        "#\n",
        "df['city']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6hu9lJXnpPs"
      },
      "source": [
        "# Then later we strip the leading and trailing spaces as shown and lastly\n",
        "# confirm our changes\n",
        "# ---\n",
        "#\n",
        "df['city'] = df['city'].str.strip()\n",
        "df['city']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgSTFXdcOuUc"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7vIG505OuUo"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Deal with the leading and trailing whitespaces from Name\n",
        "# and Team variables in the following dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/NBABasketballDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebaricL9O20B"
      },
      "source": [
        "## 5. Irrelevant Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWZ0GCbCO20L"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTzUuFAyO20Q"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# We can also deleting/dropping irrelevant columns/features.\n",
        "# By irrelevant we mean dataset features that we don't need\n",
        "# to answer a research question.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# Hint: Use the df dataframe you created earlier\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oiYYzezq4Wr"
      },
      "source": [
        "# Deleting an Irrelevant Column i.e. if we didn't require the column city\n",
        "# to answer our research question.\n",
        "# ---\n",
        "# While dropping/deleting those two columns:\n",
        "# a) We set axis = 1\n",
        "#    A dataframe has two axes: “axis 0” and “axis 1”.\n",
        "#    “axis 0” represents rows and “axis 1” represents columns.\n",
        "# b) We can also set Inplace = True.\n",
        "#    This means the changes would be made to the original dataframe.\n",
        "# Dropping the irrelevant columns i.e. Team and Weight\n",
        "# Those values were dropped since axis was set equal to 1 and\n",
        "# the changes were made in the original data frame since inplace was True.\n",
        "#\n",
        "df.drop([\"city\"], axis = 1, inplace = True)\n",
        "\n",
        "# And preview our resulting dataset\n",
        "# ---\n",
        "#\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naAOHnkNsz-y"
      },
      "source": [
        "# We can drop multiple columns as shown\n",
        "# ---\n",
        "#\n",
        "df.drop([\"height\", \"weight\"], axis = 1, inplace = True)\n",
        "\n",
        "# And preview our resulting dataset\n",
        "# ---\n",
        "#\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlGDqmxO20t"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# We can also fix in-record & cross-datasets errors.\n",
        "# These kinds errors result from having two or more values in the same row\n",
        "# or across datasets contradicting with each other.\n",
        "# ---\n",
        "# Dataset = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df['total_account_2'] = df['account_a'] + df['account_b']\n",
        "\n",
        "# Previewing our resulting dataframe\n",
        "# ---\n",
        "#\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0W-6fnvx0A"
      },
      "source": [
        "# Create another column to tell us whether if the two columns match.\n",
        "# We will use the numpy library through use of np.\n",
        "# ---\n",
        "#\n",
        "df['total_account?'] = np.where(df['total_account'] == df['total_account_2'], 'True', 'False')\n",
        "\n",
        "# Previewing our resulting dataframe\n",
        "# ---\n",
        "#\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGxUgJJ4wilM"
      },
      "source": [
        "# Let's now select the records which don't match\n",
        "# ---\n",
        "#\n",
        "df.loc[df['total_account?'] == \"False\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CeKrayqxQuL"
      },
      "source": [
        "# At this point we can do several things\n",
        "# 1. Correct the values,\n",
        "# 2. Drop/Delete the values,\n",
        "# 3. Or even decide to leave them as they are for certain reasons\n",
        "# ---\n",
        "# If we had a large dataset, we could get the no. of records using len(),\n",
        "# this would help us in our decision making process.\n",
        "# ---\n",
        "#\n",
        "len(df.loc[df['total_account?'] == \"False\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXsiMoefO21J"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18qYASsPO21L"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: While perfoming some analysis to answer a research question,\n",
        "# we realize that we don't need the Date and Time features in our dataset.\n",
        "# Let's drop those two features below\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/SuperMarketSalesDB\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQvS83K_PBJh"
      },
      "source": [
        "## 6. Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjklXwZLPBJw"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTlrpqh-PBJ5"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Finding duplicate records\n",
        "# -> Duplicate records are repeated records in a dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/NBABasketballDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "nba_df = pd.read_csv('http://bit.ly/NBABasketballDataset')\n",
        "\n",
        "# Again, we first explore our dataset by determining the shape of\n",
        "# our dataset (records/instances, columns/variables)\n",
        "# ---\n",
        "#\n",
        "nba_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRdHqs9p1L8t"
      },
      "source": [
        "# We can then identify which observations are duplicates\n",
        "# through the duplicated() function and sum() to know how many\n",
        "# duplicate records there are.\n",
        "# Normally, duplicate records are dropped from the dataset.\n",
        "# But in our case we don't have any duplicate records.\n",
        "# ---\n",
        "#\n",
        "nba_df = nba_df[nba_df.duplicated()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd6R2AB77A1e"
      },
      "source": [
        "# Finding the no. of duplicates\n",
        "# ---\n",
        "#\n",
        "sum(nba_df.duplicated())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFMDOQUCPBKg"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Dropping duplicate columns\n",
        "# ---\n",
        "# Dataset = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# In our previous dataset, if there were duplicates we\n",
        "# could have dropped them through the use of the drop_duplicates() function\n",
        "# as shown in this example\n",
        "# ---\n",
        "#\n",
        "nba_df_duplicates = nba_df.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CNl8VL-PBKp"
      },
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Dropping duplicates in a specific column in thed df dataframe\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# ---\n",
        "#\n",
        "\n",
        "# We can also consider records with repeated variables/columns\n",
        "# as duplicates and deal with them. For example, we can\n",
        "# identify duplicates in our dataset based on country.\n",
        "# ---\n",
        "#\n",
        "duplicates_df = df[df.duplicated(['country'])]\n",
        "duplicates_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D33CBMxu5agd"
      },
      "source": [
        "# Then dropping the duplicates as shown below.\n",
        "# NB: We will create in a new dataframe object which will contain our unique dataframe\n",
        "# which won't have any duplicates.\n",
        "# ---\n",
        "#\n",
        "unique_df = df.drop_duplicates(['country'])\n",
        "\n",
        "# Determining the size of our new dataset\n",
        "# We note that the two records were dropped from our original dataset\n",
        "# ---\n",
        "#\n",
        "unique_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP7c_QZcPBK0"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qxMH94BPBK4"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Find the duplicates in the following dataset.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln3Js0AFPBLC"
      },
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: From your understanding of the features, deal with the duplicates found in the given dataset.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmQzRWnBAUip"
      },
      "source": [
        "## 7. Missing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVDxacbDW-Wo"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2e_4DahAZ1L"
      },
      "source": [
        "# Example 3a\n",
        "# ---\n",
        "# Checking for missing data\n",
        "# NB: This method may not be the most convenient. Why?\n",
        "# ---\n",
        "# We can check if there is any missing values in the entire dataframe as shown\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.isnull()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzLGQemeAZtn"
      },
      "source": [
        "# Example 3b\n",
        "# ---\n",
        "# We can also check for missing values in each column\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q6GrIVILiQb"
      },
      "source": [
        "# Example 3c\n",
        "# ---\n",
        "# We can check how many missing values there are across each column by\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c26TzAbLljF"
      },
      "source": [
        "# Example 3d\n",
        "# ---\n",
        "# We can also check to see if we have any missing values in the dataframe\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "print(df.isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_qoob_9LnZP"
      },
      "source": [
        "# Example 3e: Method 1\n",
        "# ---\n",
        "# Dealing with the missing data\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# We can drop the missing observations\n",
        "# ---\n",
        "#\n",
        "df_no_missing = df.dropna()\n",
        "\n",
        "# Checking for missing data\n",
        "df_no_missing.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg4-ef-sLqlb"
      },
      "source": [
        "# Example 3e: Method 2\n",
        "# ---\n",
        "# We can drop rows where all cells in that row is NA\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df_cleaned = df.dropna(how='all')\n",
        "\n",
        "# Checking the shape of our dataset\n",
        "df_cleaned.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxfcpIWILsHF"
      },
      "source": [
        "# Example 3e: Method 3\n",
        "# ---\n",
        "# We could drop columns if they only contain missing values\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df_without_columns = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Checking the shape of our dataset\n",
        "df_without_columns.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nRHn99xLuGU"
      },
      "source": [
        "# Example 3e: Method 4\n",
        "# ---\n",
        "# We could drop rows that contain less than five observations\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "df.dropna(thresh=5)\n",
        "\n",
        "# Checking the shape of our dataset\n",
        "df.shape\n",
        "\n",
        "\n",
        "# Further reading\n",
        "# ----\n",
        "# Above are only a few methods of dealing with missing data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxBhzro9WOsp"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHVLpO6bLt9r"
      },
      "source": [
        "# Challenge 3a\n",
        "# ---\n",
        "# Question: Find the missing values in the following datset.\n",
        "# You can use any of the above methods to you see fit.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIaciSKTMCRl"
      },
      "source": [
        "# Challenge 3b\n",
        "# ---\n",
        "# Question: Handle the missing values in the following dataset.\n",
        "# You can any of the above methods that you see fit.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZOg8AbeAaub"
      },
      "source": [
        "## 8. Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFOkaq3vXZMh"
      },
      "source": [
        "#### <font color=\"blue\">Examples & Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlWrf44NAecP"
      },
      "source": [
        "# Example 4a\n",
        "# ---\n",
        "# Selecting rows when columns contain certain values\n",
        "# ---\n",
        "# NB: Selecting records where petal_length is 5.0\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Reading our csv file from the given url and storing it to a dataframe.\n",
        "iris_df = pd.read_csv(\"http://bit.ly/IrisDataset\")\n",
        "\n",
        "# Previewing the first 5 records\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CssurHCU2gXU"
      },
      "source": [
        "# Selecting the records where column has certain values\n",
        "# ---\n",
        "#\n",
        "iris_df[iris_df.petal_length.isin(['5.0'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48D2vBZuEukC"
      },
      "source": [
        "# Challenge 4a\n",
        "# ---\n",
        "# Question: From the given dataset, find observations with outlets\n",
        "# established in 2002.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQoQq-nfAjL7"
      },
      "source": [
        "# Example 4b\n",
        "# ---\n",
        "# Selecting the records where column doesn't have certain values\n",
        "# in our case, where petal_leghth is not 5.0\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "iris_df[~iris_df.petal_length.isin(['5.0'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5IPcb_bEv_B"
      },
      "source": [
        "# Challenge 4b\n",
        "# ---\n",
        "# Question: Select all the Dairy observations from the given dataset.\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDnwO9TSHy3j"
      },
      "source": [
        "# Example 4c\n",
        "# ---\n",
        "# Selecting records using filters where petal_width greater than 1.9\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "iris_df[(iris_df['petal_width'] > 1.9)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYeaoRAIH0vO"
      },
      "source": [
        "# Challenge 4c\n",
        "# ---\n",
        "# Question: Which observations had items outlet sales greater than 2000?\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhrdWafVWXkj"
      },
      "source": [
        "# Example 5d\n",
        "# ---\n",
        "# We can also use the query method to get for data where petal_width is equal to 1.0\n",
        "# Once you run this cell, replace the equals operator == to with less than <\n",
        "# or greater than > operators to see their applications as well.\n",
        "# ---\n",
        "# The parameter inplace makes changes in the original dataframe if True.\n",
        "# We should also note the query method works if the column name doesn’t have any empty spaces.\n",
        "# Hence the need replace blank spaces in our column names with '_'\n",
        "# ---\n",
        "#\n",
        "\n",
        "# Let's filter our data\n",
        "#\n",
        "iris_df.query('petal_width == 1.9', inplace = True)\n",
        "\n",
        "# Previewing our dataset\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxqszAhTZCWU"
      },
      "source": [
        "# We can also perform multiple condition filtering as shown below\n",
        "# ---\n",
        "#\n",
        "\n",
        "iris_df.query('sepal_width == 2.7 and petal_length == 5.1', inplace = True)\n",
        "\n",
        "# Previewing our dataset\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ngs4-KCdszu"
      },
      "source": [
        "# We can also get the same result as shown below\n",
        "# ---\n",
        "#\n",
        "# Or uncomment the following line\n",
        "iris_df[(iris_df.sepal_length == 5.8) & (iris_df.petal_length == 5.1)]\n",
        "\n",
        "# Previewing our dataset\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrpXmqvbd_mo"
      },
      "source": [
        "# Challenge 5d\n",
        "# ---\n",
        "# Question: Which observations had items outlet sales greater than 2000 and less than 3000?\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1tL9P5BAhWS"
      },
      "source": [
        "## 9. Sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DeWc_D7WVBl"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgC9N1_DAjq3"
      },
      "source": [
        "# Example 5a\n",
        "# ---\n",
        "# Load the given dataframe in ascending order by reports\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Sorting our dataset in ascending order by sepal_length\n",
        "# ---\n",
        "#\n",
        "iris_df.sort_values(by='sepal_length', ascending=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVTVl_GpWToR"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "too5E8HVBJSk"
      },
      "source": [
        "# Challenge 5a\n",
        "# ---\n",
        "# Sort items by weight in descending order given the following dataset.\n",
        "# Hint: ascending = 0\n",
        "# ---\n",
        "# Dataset url = https://bit.ly/ShoprityDS\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bISArgNWBxR-"
      },
      "source": [
        "## 10. Splitting, Merging and Concatenation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf72T4GKXzb_"
      },
      "source": [
        "#### <font color=\"blue\">Examples & Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zceIjjUiBy5A"
      },
      "source": [
        "# Example 6a\n",
        "# ---\n",
        "# Split the dataframe species column into two columns by using the \"-\" character\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Dropping null value columns to avoid errors\n",
        "iris_df.dropna(inplace = True)\n",
        "\n",
        "# New data frame with split value columns\n",
        "new_iris_df = iris_df[\"species\"].str.split(\"-\", n = 1, expand = True)\n",
        "\n",
        "# Naking separate first name column from new data frame\n",
        "iris_df[\"family\"]= new_iris_df[0]\n",
        "\n",
        "# Making separate last name column from new data frame\n",
        "iris_df[\"sub-species\"]= new_iris_df[1]\n",
        "\n",
        "# Dropping old Name columns\n",
        "iris_df.drop(columns =[\"species\"], inplace = True)\n",
        "\n",
        "# Displaying our dataframe\n",
        "iris_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwGSNkqz02qp"
      },
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# Concatenating (merging) two columns in a dataframe\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Concatenating our column\n",
        "#\n",
        "iris_df['species'] = iris_df['family'].str.cat(iris_df['sub-species'],sep=\"-\")\n",
        "\n",
        "# Dropping old name columns\n",
        "#\n",
        "iris_df.drop(columns = [\"family\", \"sub-species\"], inplace = True)\n",
        "\n",
        "# Previewing our new dataframe\n",
        "#\n",
        "iris_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEZyhikz3con"
      },
      "source": [
        "# Challenge 6a\n",
        "# ---\n",
        "# Concatenate the the city and state columns from the dataset below.\n",
        "# The resulting region column should have city and state seperated by a comma and whitespace.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SchoolShootingsDataset\n",
        "# ---\n",
        "# Step 1: Load the dataset\n",
        "# Step 2: Preview the dataset\n",
        "# Step 3: Perform your concatenation\n",
        "# Step 4: Dropping old columns\n",
        "# Step 4: Preview your final dataframe\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzPzBmYTPaSC"
      },
      "source": [
        "# Example 6b\n",
        "# ---\n",
        "# Merging two dataframes.\n",
        "# In this example, we will create two simple dataframes for demonstration purposes.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Let's create our first dataframe and preview it\n",
        "#\n",
        "df1 = pd.DataFrame([[2, 3], [41, 51]], columns=['a', 'b'])\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di8xE4ToPb7_"
      },
      "source": [
        " # Let our second dataframe and preview it\n",
        "#\n",
        "df2 = pd.DataFrame([[2, 5], [41, 6]], columns=['a', 'c'])\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ8fDmmkzhEl"
      },
      "source": [
        "# Merging our dataframes and storing our resulting dataframe in df3, then previewing it\n",
        "#\n",
        "df3 = df1.merge(df2, how='left', on='a')\n",
        "df3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSVuIQojFFBR"
      },
      "source": [
        "# Challenge 6b\n",
        "# ---\n",
        "# Merge the following two datasets\n",
        "# ---\n",
        "# Dataset 1 url = http://bit.ly/CitiesDataset\n",
        "# Dataset 2 url = http://bit.ly/CountriesDataset1\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5M3iGK8PZ3Y"
      },
      "source": [
        "## 11. Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnxNSUhdPZ3u"
      },
      "source": [
        "#### <font color=\"blue\">Examples</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpMrSX1PZ36"
      },
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# Given the following dataset, find and deal with outliers.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/CountryDataset1\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Let's read data from url as dataframe\n",
        "#\n",
        "outliersc_df = pd.read_csv(\"http://bit.ly/CountryDataset1\")\n",
        "\n",
        "# Lets preview our our dataframe below\n",
        "#\n",
        "outliersc_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MtbaxstdIby"
      },
      "source": [
        "# Checking the size of our dataset for cleaning purposes\n",
        "# ---\n",
        "#\n",
        "outliersc_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q33K99Jsc1wH"
      },
      "source": [
        "# There are many ways of dealing with the outliers however in this session we will\n",
        "# use the interquartile range (IQR) method.\n",
        "# The IQR is the first quartile subtracted from the third quartile,\n",
        "# i.e. the range covered by the middle 50% of the data. Values outside this range\n",
        "# will be considered as outliers. If we were to use a box plot visualisation then,\n",
        "# we would be able to visually see those values outside this range.\n",
        "# Something to note is that this method will consider only the numerical values in\n",
        "# our dataset. Lets now calculate the IQR for each column.\n",
        "# ---\n",
        "#\n",
        "\n",
        "# We first defining our quantiles using the quantile() function\n",
        "# ---\n",
        "#\n",
        "Q1 = outliersc_df.quantile(0.25)\n",
        "Q3 = outliersc_df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "IQR\n",
        "\n",
        "# Then filtering out our outliers by getting values which are outside our IQR Range.\n",
        "# ---\n",
        "#\n",
        "outliers_df_iqr = outliersc_df[((outliersc_df < (Q1 - 1.5 * IQR)) | (outliersc_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# Checking the size of the dataset with outliers for cleaning purposes\n",
        "# ---\n",
        "#\n",
        "outliers_df_iqr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWJth6qXczMC"
      },
      "source": [
        "# We can also explore our outliers by doing the following\n",
        "# ---\n",
        "#\n",
        "outliers_df_iqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjGji1pYUMTq"
      },
      "source": [
        "# Lastly, the most common method of handling our outliers is to drop them.\n",
        "# In some cases we can:\n",
        "# 1. Leave them if they are genuine\n",
        "# 2. Replace them with values within the IQR range\n",
        "# 3. Drop them\n",
        "# ---\n",
        "# In our case, we will drop them.\n",
        "# We just use the \"~\" character to refer to the other part of the dataset that\n",
        "# does not have outliers\n",
        "# ---\n",
        "#\n",
        "clean_dfc_iqr = outliersc_df[ ~((outliersc_df < (Q1 - 1.5 * IQR)) | (outliersc_df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "\n",
        "# Checking the size of our final dataset.\n",
        "clean_dfc_iqr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt8Se1hXPZ4C"
      },
      "source": [
        "#### <font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwsxAgwLPZ4Y"
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Find and Deal with the outliers in the given df dataframe.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/DataCleaningDataset\n",
        "# You can use the df dataframe you created ealier on.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAuIYctBB1ZX"
      },
      "source": [
        "## 12. Exporting Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ84H5gZYIgZ"
      },
      "source": [
        "#### <font color=\"blue\">Examples & Challenges</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMxOj6omB2a_"
      },
      "source": [
        "# Pre-requisite 7\n",
        "# ---\n",
        "# Importing files module\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Files module will allow us to download our file from colaboratory\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e83--Ka_B21z"
      },
      "source": [
        "# Example 7a\n",
        "# ---\n",
        "# Load the given dataframe in ascending order by reports\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "# Create our sample dataframe\n",
        "data = {'name': ['Daniel', 'Joyce', 'Elizabeth', 'Sanni', 'Sefu'],\n",
        "        'year': [2012, 2012, 2013, 2014, 2014],\n",
        "        'reports': [4, 24, 31, 2, 3]}\n",
        "df = pd.DataFrame(data, index = ['Nairobi', 'Cairo', 'Cape Town', 'Adis Ababa', 'Mombasa'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSqkbI0RPoq8"
      },
      "source": [
        "# Exporting our sample dataframe as a csv\n",
        "# ---\n",
        "#\n",
        "\n",
        "df.to_csv('example.csv')\n",
        "\n",
        "# NB: In order to download our files from colaboratory we need to run the following\n",
        "#\n",
        "files.download('example.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5RNQvkq3sww"
      },
      "source": [
        "# Challenge 7a\n",
        "# ---\n",
        "# Question: Export a csv file of your resulting shoprity dataframe below\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5_yqeq9Pq2h"
      },
      "source": [
        "# Example 7b\n",
        "# ---\n",
        "# Exporting our sample dataframe as a excel file (xlsx)\n",
        "# ---\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n",
        "\n",
        "df.to_excel('example.xlsx')\n",
        "\n",
        "# NB: In order to download our files from colab we need to run the following\n",
        "#\n",
        "files.download('example.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1DnZc-isbs9"
      },
      "source": [
        "# Challenge 7b\n",
        "# ---\n",
        "# Question: Export a excel file of your resulting shoprity dataframe\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}